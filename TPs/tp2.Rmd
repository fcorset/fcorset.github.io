---
title: "TP 2 : Simulation de lois de probabilités"
---


# Introduction

Dans le logiciel R, on peut simuler très simplement à peu près toutes les lois classiques. Dans ce TP, on se propose, pour chaque loi usuelle, de simuler des échantillons, et de calculer la moyenne et la variance de ces échantillons, ainsi que faire des graphiques représentant ces lois. Il y a quatre commandes à connaître pour chaque loi :

- rmaloi : permet de simuler selon maloi 
- dmaloi : permet de calculer la densité de maloi
- pmaloi : permet de calculer la fonction de répartition de maloi
- qmaloi : permet de calculer le quantile de maloi

*Exemple*

Pour la loi normale :

```{r}
n<-30
ech<-rnorm(n,mean = 0,sd = 2) # simulation d'un échantillon d'une loi normale d'espérance 1 et de variance 4 (écart-type = 2)
head(ech,n = 20)
hist(ech, freq=F)
curve(dnorm(x,mean = 0,sd = 2),from = -6,to = 6,ylab="densité",add=T,col="red")
plot(ecdf(ech))
curve(pnorm(x,mean = 0,sd = 2),from = -6,to = 6,ylab="Fonction de répartition",add=T,col="red")
quantile.normale<-qnorm(p = c(0.05,0.1,0.25,0.5,0.75,0.9,0.95),mean = 0,sd = 2)
quantile.echantillon<-quantile(x = ech,probs = c(0.05,0.1,0.25,0.5,0.75,0.9,0.95))
(rbind(quantile.echantillon,quantile.normale))
```


# Lois discrètes

## Loi uniforme discrète

Soit $X$ une variable aléatoire distribuées selon une loi uniforme discrète sur $\{1,\ldots,n\}$.

La distribution est définie comme suit :
$$
\forall k\in \{1,\ldots,n\}, \, p(X=k)=\frac 1n
$$

En R,

```{r}
n<-30
ech<-sample(1:10,n,replace=T)
table(ech)/n
barplot(table(ech)/n)
abline(h=1/10,col="red")
```

## Loi de Bernoulli

Cette loi est notée $\mathcal B(p)$. On dit que $X\leadsto \mathcal B(p)$ si et seulement si $X$ est une variable binaire telle que (on parle de loi succès/échec) :
$$
P(X=1)=p
$$
et
$$
P(X=0)=1-p=q
$$

C'est un cas particulier de la loi binomiale avec $n=1$.

On peut vérifier facilement par le calcul que $\mathbb E[X]=p$ et $\mathbb V[X] = pq = p(1-p)$.

```{r}
n<- 30  # taille de l'échantillon
p<-0.3   # paramètre de la loi de Bernoulli
esp<-p
var<-p*(1-p)
ech<-rbinom(n,size = 1,prob = p) # size = 1 correspond à la loi de Bernoulli
table(ech)/n
paste("La moyenne de l'échantillon est ",mean(ech),", sachant que l'espérance vaut",esp)
paste("La variance de l'échantillon est ",round(var(ech),2),", sachant que la variance de la loi vaut",p*(1-p))
```

## Loi binomiale

On dit que $X$ suit une loi binomiale, notée $\mathcal B(n,p)$. 

On dit que cette loi est la somme de lois de Bernoulli indépendantes et de même paramètre, ce qui revient à compter le nombre de succès (nombre de $1$) parmi les $n$ expériences. Cela revient donc à faire $n$ fois une expérience (qui a une probabilité $p$ de réussir) identique, de façon indépendante. $X$ représente donc le nombre de succès.

Pour résumer, soit $Y_i\leadsto \mathcal B(p)$ pour $i=1,\ldots,n$, où les $Y_i$ sont indépendants. On note 
$$
X = \sum_{i=1}^n Y_i \leadsto \mathcal B(n,p)
$$




Les valeurs possibles pour la variable $X$ sont $\{0,\ldots,n\}$ et 
$$
\forall k\in\{0,\ldots,n\},\, p_k=p(X=k)=C_n^k p^k (1-p)^{n-k}
$$

On peut remarquer que la somme des probabilités est bien égale à 1 grace à la formule du binôme de Newton :
$$
(a+b)^n = \sum_{k=0}^n C_n^k a^kb^{n-k}
$$
En prenant $a=p$ et $b=q=1-p$ on trouve :
$$
\sum_{k=0}^n p_k= \sum_{k=0}^nC_n^k p^k (1-p)^{n-k} = (p+1-p)^n=1^n=1
$$

L'espérance de cette loi est (grâce à la **linéarité de l'espérance**)
$$
\mathbb E[X] = \sum_{i=1}^n \mathbb E[Y_i] = \sum_{i=1}^n p  = np
$$
La variance de cette loi est
$$
\mathbb V[X] = \sum_{i=1}^n \mathbb V[Y_i] = \sum_{i=1}^n p(1-p)  = np(1-p)
$$
Attention, dans cette dernière formule, il ne faut pas croire que nous utilisons la linéarité de la variance ! **La variance n'est pas linéaire**. Pour rappel :
$$
\mathbb V[aX] = a^2 \mathbb V[X]
$$
et
$$
\mathbb V[X_1+X_2] = \mathbb V[X_1] + \mathbb V[X_2] + 2*\mathrm{cov}(X_1,X_2)
$$
Enfin, si $X_1$ et $X_2$ sont indépendantes, alors
$$
\mathrm{cov}(X_1,X_2)=0
$$

En R, cela donne
```{r}
N<- 100 # taille de l'échantillon
n<- 10L  # premier paramètre de la loi
p<- 0.8 # deuxième paramètre de la loi

ech<-rbinom(N,n,p)
table(ech)/N

dbinom(x=0:n,size = n,prob = p)
```


## Loi Géométrique

On dit que $X$ suit une loi géométrique, notée $\mathcal G(p)$ avec $0<p<1$.

Cette loi correspond au shéma suivant. On répète, de façon indépendante, une même expérience (qui a une probabilité $p$ de réussite et une probabilité $q=1-p$ d'échec) et on note $X$ le rang du premier succès.

Les valeurs prises par $X$ sont donc $X(\Omega)=\mathbb N^\star$ et les probabilités associées sont :
$$
\forall k\in\mathbb N^\star,\, p_k=P(X=k) = (1-p)^{k-1}p = q^{k-1}p
$$
On remarque que ces probabilités $p_k$ définissent une suite géométrique de raison $q$.

Pour rappel, si $0<q<1$ alors 
$$
\sum_{k=n_0}^n q^k = q^{n_0}\dfrac{1-q^{n-n_0+1}}{1-q}
$$
et donc par passage à la limite
$$
\sum_{k=n_0}^{+\infty} q^k = \dfrac{q^{n_0}}{1-q}
$$
En appliquant cette formule pour calculer la somme des probabilités, on trouve

$$
\sum_{k=1}^{+\infty} p_k = \sum_{k=1}^{+\infty} q^{k-1}p=p\sum_{k=0}^{+\infty}q^k=p/(1-q) = 1
$$

L'espérance de cette loi est :
$$
\mathbb E[X] = \sum_{k=1}^{+\infty} kp_k = p\sum_{k=1}^{+\infty}kq^{k-1} = p/(1-q)^2 = 1/p
$$
Le moment d'ordre 2 de cette loi est :
$$
\mathbb E[X^2] = \sum_{k=1}^{+\infty} k^2p_k = p\sum_{k=1}^{+\infty}k(k-1+1)q^{k-1} = pq\sum_{k=1}^{+\infty} k(k-1)q^{k-2} + p\sum_{k=1}^{+\infty}kq^{k-1} = pq\dfrac{2}{(1-q)^3}+1/p = \dfrac{2(1-p)+p}{p^2}= \dfrac{1+q}{p^2}
$$
La variance vaut
$$
\mathbb V[X] = \mathbb E[X^2] - \mathbb E[X]^2 = \dfrac{1+q}{p^2}-\dfrac{1}{p^2}=\dfrac{q}{p^2}
$$